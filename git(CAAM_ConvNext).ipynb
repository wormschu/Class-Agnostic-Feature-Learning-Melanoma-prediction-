{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6062e2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import models\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        model = models.convnext_tiny(weights=models.ConvNeXt_Tiny_Weights.IMAGENET1K_V1)\n",
    "        self.feature_extractor = torch.nn.Sequential(*list(model.features.children()))\n",
    "        \n",
    "        self.gap = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(768, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        cam_features = self.feature_extractor(x)\n",
    "        features = self.gap(cam_features)\n",
    "        features = features.view(x.shape[0], -1)\n",
    "        logits = self.classifier(features)\n",
    "        return logits,cam_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b296753",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import pickle\n",
    "from PIL import Image\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def min_max_normalize(image):\n",
    "    min_value,_ = torch.min(image, 1)\n",
    "    min_value,_ = torch.min(min_value, 1)\n",
    "    min_value = min_value.unsqueeze(1).unsqueeze(1)\n",
    "    max_value,_ = torch.max(image, 1)\n",
    "    max_value,_ = torch.max(max_value, 1)\n",
    "    max_value = max_value.unsqueeze(1).unsqueeze(1)\n",
    "    \n",
    "    output = (image - min_value) / (max_value - min_value)\n",
    "    \n",
    "    return output\n",
    "\n",
    "class CustomDataset:\n",
    "    def __init__(self, all_json, transform):\n",
    "        self.all_json = all_json\n",
    "        self.transform = transform\n",
    "        self.basedir = './ISIC_2019_Training_Input/'\n",
    "        self.label_dict = {\"NV\": 0, \"MEL\": 1}\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.basedir + self.all_json[idx]['image'] + '.jpg'\n",
    "        image = self.transform(Image.open(img_path).convert(\"RGB\"))\n",
    "        label = self.label_dict[self.all_json[idx]['label']]\n",
    "        return image, image, torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_json)\n",
    "\n",
    "\n",
    "def do_epoch1(model, dataloader, criterion, optim=None):\n",
    "    total_loss = 0\n",
    "    total_accuracy = 0\n",
    "\n",
    "    for x, _, y_true in tqdm(dataloader, leave=False):\n",
    "        x, y_true = x.to(device), y_true.to(device)\n",
    "        y_pred, _ = model(x)\n",
    "        loss = criterion(y_pred, y_true)\n",
    "\n",
    "        if optim is not None:\n",
    "            optim.zero_grad()\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_accuracy += (y_pred.argmax(dim=1) == y_true).float().mean().item()\n",
    "\n",
    "    mean_loss = total_loss / len(dataloader)\n",
    "    mean_accuracy = total_accuracy / len(dataloader)\n",
    "    return mean_loss, mean_accuracy\n",
    "\n",
    "\n",
    "def do_epoch2(model, dataloader, criterion1, criterion2, optim1=None, optim2=None):\n",
    "    total_ce1_loss = 0\n",
    "    total_ce2_loss = 0\n",
    "    total_caam_loss = 0\n",
    "    total_accuracy = 0\n",
    "    count = 0\n",
    "\n",
    "    for x1, x2, y_true in tqdm(dataloader, leave=False):\n",
    "        x1, x2, y_true = x1.to(device), x2.to(device), y_true.to(device)\n",
    "        seed = random.randint(1,1000)\n",
    "        \n",
    "        # 1. Cross-Entropy Loss1 (original input)\n",
    "        y_pred, feature_map = model(x1)\n",
    "        feature_map = F.relu(feature_map)\n",
    "        ce1_loss = criterion1(y_pred, y_true)\n",
    "\n",
    "        # 2. CAAM Loss\n",
    "        cam = min_max_normalize(torch.sum(feature_map, dim=1))\n",
    "        \n",
    "        transform_HF =  transforms.RandomHorizontalFlip()\n",
    "        transform_VF =  transforms.RandomVerticalFlip()\n",
    "        transform_random_crop =  transforms.RandomCrop(random.randint(160,210))\n",
    "        transform_resize = transforms.Resize(224)\n",
    "\n",
    "        ori_caam = transform_resize(cam)\n",
    "        torch.manual_seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        random.seed(seed)\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        # transformed ori-caam\n",
    "        caam = transform_resize(transform_VF(transform_HF(transform_random_crop(ori_caam))))\n",
    "        \n",
    "        torch.manual_seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        random.seed(seed)\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        transform_x = transform_resize(transform_VF(transform_HF(transform_random_crop(x2))))\n",
    "        ty_pred,transform_feature_map = model(transform_x)\n",
    "        transform_feature_map = F.relu(transform_feature_map)\n",
    "        \n",
    "        # transformed caam\n",
    "        transform_caam = transform_resize(min_max_normalize(torch.sum(transform_feature_map, dim=1)))\n",
    "        \n",
    "        caam_loss = criterion2(transform_caam, caam)\n",
    "        \n",
    "        # 3. Cross-Entropy Loss2 ( input)\n",
    "        ce2_loss = criterion1(ty_pred, y_true)\n",
    "        \n",
    "        loss = ce1_loss + caam_loss + ce2_loss\n",
    "        \n",
    "        if optim1 is not None:\n",
    "            optim1.zero_grad()\n",
    "            optim2.zero_grad()\n",
    "            loss.backward()\n",
    "            optim1.step()\n",
    "            optim2.step()\n",
    "        \n",
    "        total_ce1_loss += ce1_loss.item()\n",
    "        total_ce2_loss += ce2_loss.item()\n",
    "        total_caam_loss += caam_loss.item()\n",
    "        total_accuracy += (y_pred.argmax(dim=1) == y_true).float().mean().item()\n",
    "        count += 1\n",
    "\n",
    "    mean_ce1_loss = total_ce1_loss / count\n",
    "    mean_ce2_loss = total_ce2_loss / count\n",
    "    mean_caam_loss = total_caam_loss / count\n",
    "    mean_accuracy = total_accuracy / count\n",
    "    return mean_ce1_loss, mean_ce2_loss, mean_caam_loss, mean_accuracy\n",
    "\n",
    "\n",
    "# Initialize device, data, and model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "with open(\"./Train.pkl\", \"rb\") as f:\n",
    "    train_set = pickle.load(f)\n",
    "with open(\"./Valid.pkl\", \"rb\") as f:\n",
    "    val_set = pickle.load(f)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "                                   transforms.ToTensor()\n",
    "                                   ,transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
    "\n",
    "train_dataset = CustomDataset(train_set, transform)\n",
    "val_dataset = CustomDataset(val_set, transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "\n",
    "# class weight\n",
    "numoflabel_1 = 0\n",
    "numoflabel_2 = 0\n",
    "for i in range(len(train_set)):\n",
    "    if train_set[i]['label'] == \"NV\":\n",
    "        numoflabel_1 +=1\n",
    "    elif train_set[i]['label'] == \"MEL\":\n",
    "        numoflabel_2 +=1\n",
    "        \n",
    "weights = torch.tensor([numoflabel_1,numoflabel_2], dtype=torch.float32)\n",
    "weights = weights / weights.sum()\n",
    "weights = 1.0 / weights\n",
    "weights = weights / weights.sum()\n",
    "\n",
    "model = Net().to(device)\n",
    "optim = torch.optim.Adam(model.parameters(),lr=5e-5)\n",
    "optm_1 = torch.optim.Adam(model.feature_extractor.parameters(), lr=1e-4)\n",
    "optm_2 = torch.optim.Adam(model.classifier.parameters(), lr=1e-5)\n",
    "\n",
    "lr_schedule1 = torch.optim.lr_scheduler.ReduceLROnPlateau(optm_1, patience=10, verbose=True)\n",
    "lr_schedule2 = torch.optim.lr_scheduler.ReduceLROnPlateau(optm_2, patience=10, verbose=True)\n",
    "\n",
    "criterion1 = nn.CrossEntropyLoss(weight=weights.to(device))\n",
    "criterion2 = nn.MSELoss()\n",
    "best_loss = 99999999\n",
    "# Initial 2-epoch training (do_epoch1)\n",
    "for epoch in range(1, 3):\n",
    "    model.train()\n",
    "    train_loss, train_accuracy = do_epoch1(model, train_loader, criterion1, optim=optim)\n",
    "\n",
    "for epoch in range(1, 100):\n",
    "    model.train()\n",
    "    train_loss1,train_loss2, train_loss3,train_accuracy = do_epoch2(model, train_loader, criterion1, criterion2, optim1 = optm_1, optim2 = optm_2)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss1,val_loss2, val_loss3,val_accuracy = do_epoch2(model, val_loader, criterion1, criterion2)\n",
    "        \n",
    "    print(f\"Epoch {epoch:03d}: Train CE1 Loss: {train_loss1:.4f}, Train CE2 Loss: {train_loss2:.4f}, Train CAAM Loss: {train_loss3:.4f}, Acc: {train_accuracy:.4f} | \"\n",
    "          f\"Val CE1 Loss: {val_loss1:.4f}, Val CE2 Loss: {val_loss2:.4f}, Val CAAM Loss: {val_loss3:.4f}, Acc: {val_accuracy:.4f}\")\n",
    "\n",
    "    if val_loss1 + val_loss2 + val_loss3 < best_loss:\n",
    "        print('Saving model...')\n",
    "        best_loss = val_loss1 + val_loss2 + val_loss3 \n",
    "        torch.save(model.state_dict(), './GIT.pt')\n",
    "\n",
    "    lr_schedule1.step(val_loss2)\n",
    "    lr_schedule2.step(val_loss1 + val_loss3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debc737a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "4090",
   "language": "python",
   "name": "worms"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
